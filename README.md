# Audio-Visual Event Localization using Attention Mechanisms

This repository presents an **end-to-end deep learning framework for Audio-Visual Event Localization (AVEL)**.  
The model jointly processes **audio signals and video frames** using **spatial, temporal, and cross-modal attention mechanisms** to:

- **Classify audio-visual events**
- **Localize events temporally (start & end times)**
- **Visualize attention across audio, video, and their interactions**

This project is designed for **research, academic, and educational purposes** and follows modern multi-modal learning principles.

---

## âœ¨ Key Features

- âœ… Audio feature extraction using MFCC, Mel Spectrogram, Chroma & Spectral Contrast
- âœ… Video feature extraction using **MobileNetV3**
- âœ… **Spatial Attention** over video frames
- âœ… **Temporal Attention** for audio and video streams
- âœ… **Cross-Modal Attention** for audioâ€“visual alignment
- âœ… Multi-head attention-based fusion
- âœ… Multi-task learning:
  - Event Classification (28 classes)
  - Temporal Localization (start & end time)
- âœ… Attention visualization for interpretability
- âœ… Early stopping & learning rate scheduling

---

## ğŸ§  Architecture Overview

The following diagrams illustrate the complete **Audio-Visual Event Localization pipeline**, including feature extraction, attention-based fusion, and output prediction.

### Overall Audio-Visual Architecture
![Overall Architecture](architecture_overview.png)

### Multi-Modal Attention Fusion
![Attention Fusion](assets/fusion.png)

### Detailed Processing Pipeline
![Detailed Pipeline](assets/Detailed Pipeline.png)

> The architecture integrates spatial, temporal, and cross-modal attention mechanisms to achieve robust and interpretable audio-visual event localization.

---

## ğŸ§© Model Architecture (Conceptual)

### 1. Audio Processing Pipeline
- Audio extraction from video
- Feature computation:
  - MFCC
  - Mel Spectrogram
  - Chroma
  - Spectral Contrast
  - Tonnetz
- 1D CNN-based Audio Encoder
- Temporal Self-Attention

### 2. Video Processing Pipeline
- Frame extraction (8 key frames per video)
- Image resizing (224 Ã— 224)
- Feature extraction using **MobileNetV3**
- Spatial Attention
- Temporal Self-Attention

### 3. Audio-Visual Fusion
- Self-attention within each modality
- Cross-modal attention (Audio â†” Video)
- Multi-head attention (8 heads)

### 4. Output Heads
- **Classification Head** â†’ Event Category (28 classes)
- **Temporal Regression Head** â†’ Start & End Time

---

## ğŸ“‚ Dataset Structure

Expected directory structure:

```text
AVE_Processed/
â”œâ”€â”€ audio_wav/
â”‚   â”œâ”€â”€ video_001.wav
â”‚   â”œâ”€â”€ video_002.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ video_frames/
â”‚   â”œâ”€â”€ video_001/
â”‚   â”‚   â”œâ”€â”€ frame_001.jpg
â”‚   â”‚   â”œâ”€â”€ frame_002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Finalave/
â”‚   â””â”€â”€ attention_visualization/
```
## ğŸ“ Annotations File

File: Annotations.txt
Format:
Category & VideoID & Quality & StartTime & EndTime

## âš™ï¸ Configuration

All hyperparameters are controlled through a central Config class:
class Config:
    num_classes = 28
    batch_size = 4
    learning_rate = 1e-4
    epochs = 30
    num_frames = 8
    audio_feat_dim = 256
    video_feat_dim = 256
Modify this class to experiment with different settings.

## ğŸ’¯ Training the Model

To train the model, run:
python ave.py

Training Features

âœ” Automatic best-model saving
âœ” Early stopping to prevent overfitting
âœ” Learning rate scheduler
âœ” Loss curve visualization

Saved model:
best_model_with_attention.pth

## ğŸ“Š Loss Functions

Classification Loss: Cross Entropy Loss
Temporal Localization Loss: L1 Loss

Final Loss:
Total Loss = Classification Loss + 0.3 Ã— Temporal Loss

## ğŸ” Attention Visualization

The model generates attention visualizations to improve interpretability:

Spatial attention heatmaps on video frames
Temporal attention plots for audio & video
Cross-modal attention alignment maps
Training loss curves

Saved at:
Finalave/attention_visualization/

## ğŸ›  Dependencies

Install required libraries using:
pip install torch torchvision librosa opencv-python numpy pandas matplotlib seaborn scikit-learn pillow

## ğŸ’» Hardware Support

âœ” CUDA-enabled GPU support
âœ” CPU fallback available
âœ” Headless server compatible (Matplotlib Agg backend)
